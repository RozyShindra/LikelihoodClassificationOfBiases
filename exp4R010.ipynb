{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Current columns in the dataset: ['text', 'original', 'labels', 'class_type', 'turker_gender', 'episode_done', 'confidence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # use for data manipulation and analysis\n",
    "import numpy as np # use for multi-dimensional array and matrix\n",
    "\n",
    "import seaborn as sns # use for high-level interface for drawing attractive and informative statistical graphics \n",
    "import matplotlib.pyplot as plt # It provides an object-oriented API for embedding plots into applications\n",
    "%matplotlib inline \n",
    "# It sets the backend of matplotlib to the 'inline' backend:\n",
    "\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB # nlp algo use to Positive , Negative comment\n",
    "\n",
    "from sklearn.model_selection import train_test_split # spliting the data between feature and target\n",
    "from sklearn.metrics import classification_report # gives whole report about metrics (e.g, recall,precision,f1_score,c_m)\n",
    "from sklearn.metrics import confusion_matrix # gives info about actual and predict\n",
    "from nltk.tokenize import RegexpTokenizer # regexp tokenizers use to split words from text  \n",
    "from nltk.stem.snowball import SnowballStemmer # stemmes words\n",
    "from sklearn.feature_extraction.text import CountVectorizer # create sparse matrix of words using regexptokenizes  \n",
    "from sklearn.pipeline import make_pipeline # use for combining all prerocessors techniuqes and algos\n",
    "\n",
    "from PIL import Image # getting images in notebook\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator# creates words cloud\n",
    "\n",
    "\n",
    "import pickle# use to dump model \n",
    "\n",
    "import warnings # ignores pink warnings \n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-28 10:27:37.321726: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-09-28 10:27:37.321779: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-09-28 10:27:37.336265: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-09-28 10:27:38.539911: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-28 10:27:43.478967: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the Multi-Dimensional Gender Bias dataset from Hugging Face\n",
    "dataset = load_dataset(\"md_gender_bias\")\n",
    "\n",
    "\n",
    "# Prepare data\n",
    "data = pd.DataFrame(dataset[\"train\"])\n",
    "X = data[\"text\"]\n",
    "y = data[\"labels\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dataframe of classes counts\n",
    "label_counts = pd.DataFrame(data.labels.value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer # regexp tokenizers use to split words from text\n",
    "tokenizer = RegexpTokenizer(r'[A-Za-z]+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'He designed monumental Lovissa War Cemetery in 1920.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting words tokenized ...\n"
     ]
    }
   ],
   "source": [
    "print('Getting words tokenized ...')\n",
    "data['text_tokenized'] = data.text.map(lambda t: tokenizer.tokenize(t)) # doing with all rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer # stemmes words\n",
    "stemmer = SnowballStemmer(\"english\") # choose a language\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting words stemmed ...\n"
     ]
    }
   ],
   "source": [
    "print('Getting words stemmed ...')\n",
    "data['text_stemmed'] = data['text_tokenized'].map(lambda l: [stemmer.stem(word) for word in l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>original</th>\n",
       "      <th>labels</th>\n",
       "      <th>class_type</th>\n",
       "      <th>turker_gender</th>\n",
       "      <th>episode_done</th>\n",
       "      <th>confidence</th>\n",
       "      <th>text_tokenized</th>\n",
       "      <th>text_stemmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>623</th>\n",
       "      <td>is the school structure intact till now Sir? I...</td>\n",
       "      <td>Is the school structure intact till now? i am ...</td>\n",
       "      <td>[3]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>certain</td>\n",
       "      <td>[is, the, school, structure, intact, till, now...</td>\n",
       "      <td>[is, the, school, structur, intact, till, now,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1776</th>\n",
       "      <td>she didnt catch it earlier, i cant get enough ...</td>\n",
       "      <td>she didnt catch it earlier, i cant get enough ...</td>\n",
       "      <td>[3]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>pretty sure</td>\n",
       "      <td>[she, didnt, catch, it, earlier, i, cant, get,...</td>\n",
       "      <td>[she, didnt, catch, it, earlier, i, cant, get,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2341</th>\n",
       "      <td>Cool, well I was born in France among pretty g...</td>\n",
       "      <td>Cool, well I was born in France, then I moved ...</td>\n",
       "      <td>[2]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>certain</td>\n",
       "      <td>[Cool, well, I, was, born, in, France, among, ...</td>\n",
       "      <td>[cool, well, i, was, born, in, franc, among, p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1557</th>\n",
       "      <td>in 2009, she also played the epic title told i...</td>\n",
       "      <td>In 2009, she also played the title told in Evi...</td>\n",
       "      <td>[0]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>certain</td>\n",
       "      <td>[in, she, also, played, the, epic, title, told...</td>\n",
       "      <td>[in, she, also, play, the, epic, titl, told, i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>828</th>\n",
       "      <td>I like a broad spectrum of authors,. You do li...</td>\n",
       "      <td>I like a broad spectrum of authors. Do you lik...</td>\n",
       "      <td>[3]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>certain</td>\n",
       "      <td>[I, like, a, broad, spectrum, of, authors, You...</td>\n",
       "      <td>[i, like, a, broad, spectrum, of, author, you,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  \\\n",
       "623   is the school structure intact till now Sir? I...   \n",
       "1776  she didnt catch it earlier, i cant get enough ...   \n",
       "2341  Cool, well I was born in France among pretty g...   \n",
       "1557  in 2009, she also played the epic title told i...   \n",
       "828   I like a broad spectrum of authors,. You do li...   \n",
       "\n",
       "                                               original labels  class_type  \\\n",
       "623   Is the school structure intact till now? i am ...    [3]           1   \n",
       "1776  she didnt catch it earlier, i cant get enough ...    [3]           1   \n",
       "2341  Cool, well I was born in France, then I moved ...    [2]           1   \n",
       "1557  In 2009, she also played the title told in Evi...    [0]           0   \n",
       "828   I like a broad spectrum of authors. Do you lik...    [3]           1   \n",
       "\n",
       "      turker_gender  episode_done   confidence  \\\n",
       "623               0          True      certain   \n",
       "1776              0          True  pretty sure   \n",
       "2341              1          True      certain   \n",
       "1557              0          True      certain   \n",
       "828               1          True      certain   \n",
       "\n",
       "                                         text_tokenized  \\\n",
       "623   [is, the, school, structure, intact, till, now...   \n",
       "1776  [she, didnt, catch, it, earlier, i, cant, get,...   \n",
       "2341  [Cool, well, I, was, born, in, France, among, ...   \n",
       "1557  [in, she, also, played, the, epic, title, told...   \n",
       "828   [I, like, a, broad, spectrum, of, authors, You...   \n",
       "\n",
       "                                           text_stemmed  \n",
       "623   [is, the, school, structur, intact, till, now,...  \n",
       "1776  [she, didnt, catch, it, earlier, i, cant, get,...  \n",
       "2341  [cool, well, i, was, born, in, franc, among, p...  \n",
       "1557  [in, she, also, play, the, epic, titl, told, i...  \n",
       "828   [i, like, a, broad, spectrum, of, author, you,...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text              0\n",
       "original          0\n",
       "labels            0\n",
       "class_type        0\n",
       "turker_gender     0\n",
       "episode_done      0\n",
       "confidence        0\n",
       "text_tokenized    0\n",
       "text_stemmed      0\n",
       "text_sent         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting joiningwords ...\n"
     ]
    }
   ],
   "source": [
    "print('Getting joiningwords ...')\n",
    "data['text_sent'] = data['text_stemmed'].map(lambda l: ' '.join(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # sliceing classes\n",
    "# if (data.labels == 0 ) or (data.labels == 0) :\n",
    "#     negative_comment = data['text']\n",
    "# elif  (data.labels == 5) or (data.labels == 4):\n",
    "#     positive_comment = data['text']\n",
    "\n",
    "# else :\n",
    "#     neutral_comment = data['text']\n",
    "# negative_comment = data[(data.labels == 0 ) or (data.labels == 0) ]\n",
    "# positive_comment = data[(data.labels == 5) or (data.labels == 4)]\n",
    "# neutral_comment= data[data.labels == 3]\n",
    "# irrelevant_comment = data[data.sentiment == 'Irrelevant']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create cv object\n",
    "cv = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = cv.fit_transform(data.text_sent) #transform all text which we tokenize and stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature[:5].toarray() # convert sparse matrix into array to print transformed features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX, testX, trainY, testY = train_test_split(feature, data.labels)  # spliting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create mnb object\n",
    "mnb = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "You appear to be using a legacy multi-label data representation. Sequence of sequences are no longer supported; use a binary array or sparse matrix instead - the MultiLabelBinarizer transformer can convert to this format.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/workspaces/LikelihoodClassificationOfBiases/exp4R010.ipynb Cell 20\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://codespaces%2Bfictional-yodel-467gwqx5w77f5rvw/workspaces/LikelihoodClassificationOfBiases/exp4R010.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m mnb\u001b[39m.\u001b[39;49mfit(trainX,trainY)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1144\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m   1146\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m   1147\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1149\u001b[0m     )\n\u001b[1;32m   1150\u001b[0m ):\n\u001b[0;32m-> 1151\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/naive_bayes.py:749\u001b[0m, in \u001b[0;36m_BaseDiscreteNB.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    746\u001b[0m _, n_features \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mshape\n\u001b[1;32m    748\u001b[0m labelbin \u001b[39m=\u001b[39m LabelBinarizer()\n\u001b[0;32m--> 749\u001b[0m Y \u001b[39m=\u001b[39m labelbin\u001b[39m.\u001b[39;49mfit_transform(y)\n\u001b[1;32m    750\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclasses_ \u001b[39m=\u001b[39m labelbin\u001b[39m.\u001b[39mclasses_\n\u001b[1;32m    751\u001b[0m \u001b[39mif\u001b[39;00m Y\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[1;32m    139\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 140\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39;49m, X, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    141\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m    142\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    143\u001b[0m         return_tuple \u001b[39m=\u001b[39m (\n\u001b[1;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[39m0\u001b[39m], X, \u001b[39mself\u001b[39m),\n\u001b[1;32m    145\u001b[0m             \u001b[39m*\u001b[39mdata_to_wrap[\u001b[39m1\u001b[39m:],\n\u001b[1;32m    146\u001b[0m         )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/preprocessing/_label.py:329\u001b[0m, in \u001b[0;36mLabelBinarizer.fit_transform\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit_transform\u001b[39m(\u001b[39mself\u001b[39m, y):\n\u001b[1;32m    310\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Fit label binarizer/transform multi-class labels to binary labels.\u001b[39;00m\n\u001b[1;32m    311\u001b[0m \n\u001b[1;32m    312\u001b[0m \u001b[39m    The output of transform is sometimes referred to as\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[39m        will be of CSR format.\u001b[39;00m\n\u001b[1;32m    328\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 329\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit(y)\u001b[39m.\u001b[39mtransform(y)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1144\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m   1146\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m   1147\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1149\u001b[0m     )\n\u001b[1;32m   1150\u001b[0m ):\n\u001b[0;32m-> 1151\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/preprocessing/_label.py:296\u001b[0m, in \u001b[0;36mLabelBinarizer.fit\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msparse_output \u001b[39mand\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpos_label \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mneg_label \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m):\n\u001b[1;32m    290\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    291\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mSparse binarization is only supported with non \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    292\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mzero pos_label and zero neg_label, got \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    293\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpos_label=\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpos_label\u001b[39m}\u001b[39;00m\u001b[39m and neg_label=\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mneg_label\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    294\u001b[0m     )\n\u001b[0;32m--> 296\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39my_type_ \u001b[39m=\u001b[39m type_of_target(y, input_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39my\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m    298\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mmultioutput\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39my_type_:\n\u001b[1;32m    299\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    300\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mMultioutput target data is not supported with label binarization\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    301\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/multiclass.py:346\u001b[0m, in \u001b[0;36mtype_of_target\u001b[0;34m(y, input_name)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    341\u001b[0m     \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    342\u001b[0m         \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(y[\u001b[39m0\u001b[39m], \u001b[39m\"\u001b[39m\u001b[39m__array__\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    343\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(y[\u001b[39m0\u001b[39m], Sequence)\n\u001b[1;32m    344\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(y[\u001b[39m0\u001b[39m], \u001b[39mstr\u001b[39m)\n\u001b[1;32m    345\u001b[0m     ):\n\u001b[0;32m--> 346\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    347\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mYou appear to be using a legacy multi-label data\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    348\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m representation. Sequence of sequences are no\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    349\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m longer supported; use a binary array or sparse\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    350\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m matrix instead - the MultiLabelBinarizer\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    351\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m transformer can convert to this format.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    352\u001b[0m         )\n\u001b[1;32m    353\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mIndexError\u001b[39;00m:\n\u001b[1;32m    354\u001b[0m     \u001b[39mpass\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: You appear to be using a legacy multi-label data representation. Sequence of sequences are no longer supported; use a binary array or sparse matrix instead - the MultiLabelBinarizer transformer can convert to this format."
     ]
    }
   ],
   "source": [
    "mnb.fit(trainX,trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "# SVM Classifier\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=1000)\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "svm_classifier = SVC()\n",
    "svm_classifier.fit(X_train_tfidf, y_train)\n",
    "svm_predictions = svm_classifier.predict(X_test_tfidf)\n",
    "\n",
    "print(\"SVM Classifier:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, svm_predictions))\n",
    "print(classification_report(y_test, svm_predictions))\n",
    "\n",
    "# Random Forest Classifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100)\n",
    "rf_classifier.fit(X_train_tfidf, y_train)\n",
    "rf_predictions = rf_classifier.predict(X_test_tfidf)\n",
    "\n",
    "print(\"Random Forest Classifier:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, rf_predictions))\n",
    "print(classification_report(y_test, rf_predictions))\n",
    "\n",
    "# ANN with Hugging Face Transformers\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "ann_classifier = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)\n",
    "ann_predictions = ann_classifier(X_test.tolist(), truncation=True, padding=True)\n",
    "\n",
    "ann_predictions = [entry['label'] for entry in ann_predictions]\n",
    "\n",
    "print(\"Artificial Neural Network (BERT):\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, ann_predictions))\n",
    "print(classification_report(y_test, ann_predictions))\n",
    "\n",
    "# RNN with PyTorch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch import nn, optim\n",
    "import torch\n",
    "\n",
    "# Tokenize and encode text data\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "encoded_texts = tokenizer(list(X_train), padding=True, truncation=True, return_tensors='pt')\n",
    "X_train_tensors = encoded_texts['input_ids']\n",
    "X_train_masks = encoded_texts['attention_mask']\n",
    "\n",
    "encoded_texts = tokenizer(list(X_test), padding=True, truncation=True, return_tensors='pt')\n",
    "X_test_tensors = encoded_texts['input_ids']\n",
    "X_test_masks = encoded_texts['attention_mask']\n",
    "\n",
    "# Create PyTorch DataLoader\n",
    "batch_size = 32\n",
    "train_data = TensorDataset(X_train_tensors, X_train_masks, torch.tensor(y_train.values))\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_data = TensorDataset(X_test_tensors, X_test_masks, torch.tensor(y_test.values))\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "# RNN Model\n",
    "class RNNClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RNNClassifier, self).__init__()\n",
    "        self.bert = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fc1 = nn.Linear(768, 2)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        output = outputs.logits\n",
    "        return output\n",
    "\n",
    "rnn_model = RNNClassifier()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(rnn_model.parameters(), lr=1e-5)\n",
    "\n",
    "# Training loop\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "rnn_model.to(device)\n",
    "for epoch in range(3):\n",
    "    rnn_model.train()\n",
    "    for batch in train_loader:\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = rnn_model(input_ids, attention_mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Evaluation\n",
    "rnn_model.eval()\n",
    "rnn_predictions = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids, attention_mask, _ = batch\n",
    "        input_ids, attention_mask = input_ids.to(device), attention_mask.to(device)\n",
    "        outputs = rnn_model(input_ids, attention_mask)\n",
    "        predictions = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "        rnn_predictions.extend(predictions)\n",
    "\n",
    "print(\"Recurrent Neural Network (BERT):\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, rnn_predictions))\n",
    "print(classification_report(y_test, rnn_predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# SVM Classifier\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=1000)\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "svm_classifier = SVC()\n",
    "svm_classifier.fit(X_train_tfidf, y_train)\n",
    "svm_predictions = svm_classifier.predict(X_test_tfidf)\n",
    "\n",
    "print(\"SVM Classifier:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, svm_predictions))\n",
    "print(classification_report(y_test, svm_predictions))\n",
    "\n",
    "# Random Forest Classifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100)\n",
    "rf_classifier.fit(X_train_tfidf, y_train)\n",
    "rf_predictions = rf_classifier.predict(X_test_tfidf)\n",
    "\n",
    "print(\"Random Forest Classifier:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, rf_predictions))\n",
    "print(classification_report(y_test, rf_predictions))\n",
    "\n",
    "# ANN with Hugging Face Transformers\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "ann_classifier = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)\n",
    "ann_predictions = ann_classifier(X_test.tolist(), truncation=True, padding=True)\n",
    "\n",
    "ann_predictions = [entry['label'] for entry in ann_predictions]\n",
    "\n",
    "print(\"Artificial Neural Network (BERT):\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, ann_predictions))\n",
    "print(classification_report(y_test, ann_predictions))\n",
    "\n",
    "# RNN with PyTorch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch import nn, optim\n",
    "import torch\n",
    "\n",
    "# Tokenize and encode text data\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "encoded_texts = tokenizer(list(X_train), padding=True, truncation=True, return_tensors='pt')\n",
    "X_train_tensors = encoded_texts['input_ids']\n",
    "X_train_masks = encoded_texts['attention_mask']\n",
    "\n",
    "encoded_texts = tokenizer(list(X_test), padding=True, truncation=True, return_tensors='pt')\n",
    "X_test_tensors = encoded_texts['input_ids']\n",
    "X_test_masks = encoded_texts['attention_mask']\n",
    "\n",
    "# Create PyTorch DataLoader\n",
    "batch_size = 32\n",
    "train_data = TensorDataset(X_train_tensors, X_train_masks, torch.tensor(y_train.values))\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_data = TensorDataset(X_test_tensors, X_test_masks, torch.tensor(y_test.values))\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "# RNN Model\n",
    "class RNNClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RNNClassifier, self).__init__()\n",
    "        self.bert = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fc1 = nn.Linear(768, 2)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        output = outputs.logits\n",
    "        return output\n",
    "\n",
    "rnn_model = RNNClassifier()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(rnn_model.parameters(), lr=1e-5)\n",
    "\n",
    "# Training loop\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "rnn_model.to(device)\n",
    "for epoch in range(3):\n",
    "    rnn_model.train()\n",
    "    for batch in train_loader:\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = rnn_model(input_ids, attention_mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Evaluation\n",
    "rnn_model.eval()\n",
    "rnn_predictions = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids, attention_mask, _ = batch\n",
    "        input_ids, attention_mask = input_ids.to(device), attention_mask.to(device)\n",
    "        outputs = rnn_model(input_ids, attention_mask)\n",
    "        predictions = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "        rnn_predictions.extend(predictions)\n",
    "\n",
    "print(\"Recurrent Neural Network (BERT):\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, rnn_predictions))\n",
    "print(classification_report(y_test, rnn_predictions))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
